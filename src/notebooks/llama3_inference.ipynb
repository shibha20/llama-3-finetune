{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\ntoken = user_secrets.get_secret(\"GITHUB_KEY\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://{token}@github.com/shreeya-dhakal/llama-3-finetune.git\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/llama-3-finetune","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git checkout -b Update-Eval-Notebook","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull origin Update-Eval-Notebook","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!mamba install --force-reinstall aiohttp -y\n!pip install -U \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n\n# Temporary fix for https://github.com/huggingface/datasets/issues/6753\n!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nimport sys\nimport os\nsys.path.append('/kaggle/working/llama-3-finetune/src/')\nfrom data.data_sampler import sample_data\nfrom unsloth import FastLanguageModel\nimport torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split_ratio = 0.2\nseed = 42","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_name = \"CohereForAI/aya_dataset\"\ndataset_type = \"aya\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lang_multi = \"Nepali\"\n_, aya_nepali_test = sample_data(dataset_name, dataset_type, split_ratio, seed, output_dir=None, lang_multi=lang_multi)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aya_nepali_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lang_multi = \"Hindi\"\n_, aya_hindi_test = sample_data(dataset_name, dataset_type, split_ratio, seed, output_dir=None, lang_multi=lang_multi)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aya_hindi_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_name = \"Saugatkafley/alpaca-nepali-sft\"\n_, alpaca_nepali_test = sample_data(dataset_name, \"alpaca\", split_ratio, seed, output_dir=None, lang_multi=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpaca_nepali_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_name = \"iamshnoo/alpaca-cleaned-hindi\"\n_, alpaca_hindi_test = sample_data(dataset_name, \"alpaca\", split_ratio, seed, output_dir=None, lang_multi=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpaca_hindi_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpaca_prompt_hindi = \"\"\"Below is an instruction in hindi that describes a task, paired with an input also in hindi that provides further context. Write a response in hindi that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpaca_prompt_nepali = \"\"\"Below is an instruction in nepali that describes a task, paired with an input also in nepali that provides further context. Write a response in nepali that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\ndef run_inference(dataset, prompt, is_alpaca_format=True):\n    print(\"HERE TEST\")\n    for i, row in enumerate(dataset):\n        if is_alpaca_format:\n            instruction = row[\"instruction\"]\n            input_text = row[\"input\"]\n            prompt = prompt.format(instruction, input_text, \"\")\n        else:\n            inputs = row[\"inputs\"]\n            prompt = prompt.format(inputs, \"\")\n        inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n\n        outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n        generated_text = tokenizer.batch_decode(outputs)[0]\n        dataset[i][\"output\"] = generated_text\n        print(generated_text)\n    return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aya_prompt_nepali = \"\"\"Below is an input also in nepali that is an instruction or context. Write a response to the instruction or add more to the context in nepali that appropriately completes the request.\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# alpaca_hindi_test_s = alpaca_hindi_test.select(range(10)) \n# run_inference(alpaca_hindi_test_s, alpaca_prompt_hindi, is_alpaca_format=True)\naya_nepali_test_s = aya_nepali_test.select(range(10)) \naya_nepali_test_s = run_inference(aya_nepali_test_s, aya_prompt_nepali, is_alpaca_format=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aya_nepali_test_s.to_csv(\"aya_nepali_test_s.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-06-07T07:22:28.633119Z","iopub.execute_input":"2024-06-07T07:22:28.633505Z","iopub.status.idle":"2024-06-07T07:22:28.955600Z","shell.execute_reply.started":"2024-06-07T07:22:28.633475Z","shell.execute_reply":"2024-06-07T07:22:28.954289Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43maya_nepali_test_s\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maya_nepali_test_s.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'aya_nepali_test_s' is not defined"],"ename":"NameError","evalue":"name 'aya_nepali_test_s' is not defined","output_type":"error"}]}]}