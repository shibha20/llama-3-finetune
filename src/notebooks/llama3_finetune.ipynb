{"metadata":{"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"- Fine-tune multi-lingual - with both Hindi and Nepali\n- Fine tune with Hindi run inference on Nepali \n- Fine tune with Nepali run inference on Hindi\n- Fine tune with Nepali run inference on Nepali\n- Fine tune with Hindi run inference on Hindi","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n","metadata":{"execution":{"iopub.status.busy":"2024-06-16T21:21:43.097756Z","iopub.execute_input":"2024-06-16T21:21:43.098271Z","iopub.status.idle":"2024-06-16T21:21:43.114417Z","shell.execute_reply.started":"2024-06-16T21:21:43.098246Z","shell.execute_reply":"2024-06-16T21:21:43.113684Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\ntoken = user_secrets.get_secret(\"GITHUB_KEY\")","metadata":{"execution":{"iopub.status.busy":"2024-06-16T21:21:45.923091Z","iopub.execute_input":"2024-06-16T21:21:45.924052Z","iopub.status.idle":"2024-06-16T21:21:46.089602Z","shell.execute_reply.started":"2024-06-16T21:21:45.923996Z","shell.execute_reply":"2024-06-16T21:21:46.088685Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!git clone https://{token}@github.com/shreeya-dhakal/llama-3-finetune.git","metadata":{"execution":{"iopub.status.busy":"2024-06-16T21:21:47.986907Z","iopub.execute_input":"2024-06-16T21:21:47.988098Z","iopub.status.idle":"2024-06-16T21:21:52.672410Z","shell.execute_reply.started":"2024-06-16T21:21:47.988056Z","shell.execute_reply":"2024-06-16T21:21:52.671320Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Cloning into 'llama-3-finetune'...\nremote: Enumerating objects: 109, done.\u001b[K\nremote: Counting objects: 100% (109/109), done.\u001b[K\nremote: Compressing objects: 100% (86/86), done.\u001b[K\nremote: Total 109 (delta 53), reused 59 (delta 15), pack-reused 0\u001b[K\nReceiving objects: 100% (109/109), 27.41 MiB | 17.30 MiB/s, done.\nResolving deltas: 100% (53/53), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%%capture\n!mamba install --force-reinstall aiohttp -y\n!pip install -U \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n\n# Temporary fix for https://github.com/huggingface/datasets/issues/6753\n!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0","metadata":{"execution":{"iopub.status.busy":"2024-06-16T21:21:52.674308Z","iopub.execute_input":"2024-06-16T21:21:52.674606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport wandb\n\nos.environ[\"WANDB_PROJECT\"]=\"llama-finetune-nepali\"\nos.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\nwandb_token = user_secrets.get_secret(\"WANDB_KEY\")\nwandb.login(key= wandb_token)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset, load_metric, concatenate_datasets\nimport sys\nimport os\nsys.path.append('/kaggle/working/llama-3-finetune/src/')\nfrom data.data_sampler import sample_data\nfrom unsloth import FastLanguageModel\nimport torch\nimport re","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split_ratio = 0.2\nseed = 42","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_name = \"CohereForAI/aya_dataset\"\naya_dataset = load_dataset(dataset_name)\ndataset_type = \"aya\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lang_multi = \"Nepali\"\naya_nepali_train, _ = sample_data(dataset_name, dataset_type, split_ratio, seed, output_dir=None, lang_multi=lang_multi)\naya_nepali_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lang_multi = \"Hindi\"\naya_hindi_train, _ = sample_data(dataset_name, dataset_type, split_ratio, seed, output_dir=None, lang_multi=lang_multi)\naya_hindi_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aya_nepali_train = aya_nepali_train.rename_columns(\n            {\"inputs\": \"input\", \"targets\": \"output\"})\nnew_column_data = [\"\"] * len(aya_nepali_train)  # Create a list with the same length as the dataset\naya_nepali_train = aya_nepali_train.add_column(\"instruction\", new_column_data)\naya_nepali_train = aya_nepali_train.remove_columns(['language', 'language_code', 'annotation_type', 'user_id'])\naya_nepali_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aya_hindi_train = aya_hindi_train.rename_columns(\n            {\"inputs\": \"input\", \"targets\": \"output\"})\nnew_column_data = [\"\"] * len(aya_hindi_train)\naya_hindi_train = aya_hindi_train.add_column(\"instruction\", new_column_data)\naya_hindi_train = aya_hindi_train.remove_columns(['language', 'language_code', 'annotation_type', 'user_id'])\naya_hindi_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_name = \"Saugatkafley/alpaca-nepali-sft\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpaca_nepali_train, _ = sample_data(dataset_name, \"alpaca\", split_ratio, seed, output_dir=None, lang_multi=None)\nalpaca_nepali_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_name = \"iamshnoo/alpaca-cleaned-hindi\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpaca_hindi_train, _ = sample_data(dataset_name, \"alpaca\", split_ratio, seed, output_dir=None, lang_multi=None)\nalpaca_hindi_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nepali_train = concatenate_datasets([aya_nepali_train, alpaca_nepali_train])\nnepali_train = nepali_train.shuffle(seed=42)\nnepali_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hindi_train = concatenate_datasets([aya_hindi_train, alpaca_hindi_train])\nhindi_train = hindi_train.shuffle(seed=42)\nhindi_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# both_train = concatenate_datasets([nepali_train, hindi_train])\n# both_train = both_train.shuffle(seed=42)\n# both_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Nepali\nprompt = \"\"\"Below is an instruction in Nepali that describes a task, paired with an input also in Nepali that provides further context. Write a response in Nepali that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#LoRA\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n\n\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\ntrain_data = nepali_train\ntrain_data = train_data.map(formatting_prompts_func, batched = True,)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_data,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n#         max_steps = 1000,                #### Setting max_steps for 1000. (1-1000)\n        learning_rate = 2e-4,\n        fp16 = not torch.cuda.is_bf16_supported(),\n        bf16 = torch.cuda.is_bf16_supported(),\n        logging_steps = 1,\n        save_steps=500,                 ### Checkpoint will be save after every 500 steps\n        optim = \"adamw_8bit\",\n        num_train_epochs=2,\n        weight_decay = 0.01,\n        report_to=\"wandb\", \n        run_name=\"llama3_alpaca_nep\", \n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs_alpaca_nep\",   # Saving the checkpoints to outputs folder\n    ),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}