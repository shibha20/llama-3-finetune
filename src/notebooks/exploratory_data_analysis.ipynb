{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreeyadhakal/Desktop/Projects-2023/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aya Dataset\n",
    "\n",
    "***Description from [HuggingFace](https://huggingface.co/datasets/CohereForAI/aya_dataset):***\n",
    "    The Aya Dataset is a multilingual instruction fine-tuning dataset curated by an open-science community via Aya Annotation Platform from Cohere For AI. The dataset contains a total of 204k human-annotated prompt-completion pairs along with the demographics data of the annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'targets', 'language', 'language_code', 'annotation_type', 'user_id'],\n",
       "        num_rows: 202362\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['inputs', 'targets', 'language', 'language_code', 'annotation_type', 'user_id'],\n",
       "        num_rows: 1750\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aya_dataset = load_dataset(\"CohereForAI/aya_dataset\")\n",
    "aya_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'targets', 'language', 'language_code', 'annotation_type', 'user_id'],\n",
       "        num_rows: 4002\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['inputs', 'targets', 'language', 'language_code', 'annotation_type', 'user_id'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aya_nepali = aya_dataset.filter(lambda example: example['language'] == \"Nepali\")\n",
    "aya_nepali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "दिइएको कवितांश पढ्नुहोस्‌ र सङ्क्षिप्त उत्तर लेख्नुहोस्‌ :\n",
      "\n",
      "डिगर्चामा डोब तिम्रो चिसो हिउँभित्र होला वेत्रावती किनारभरि पौरखको\n",
      "चिनो होला वीर पुर्खा! तिमीलाई मितेरीले मात्र बाँध्यो सागर तरी\n",
      "संसारभरि वीर गोर्खा रगत बग्यो।\n",
      "\n",
      "प्रश्न : हामीले हाम्रा पुर्खाको गौरव कसरी जोगाउन सक्छौं ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(aya_nepali['train']['inputs'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "उत्तरः पुर्खाले गरेको त्याग, तपस्या र बलिदानको उच्च सम्मान गर्दै एवम्‌\n",
      "देशभक्तिपूर्ण बाटोलाई तन, मन, वचन तथा कर्मले आत्मसात्‌ गरी हाम्रा\n",
      "पुर्खाको गौरव जोगाउन सक्छौं ।\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(aya_nepali['train']['targets'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 4002/4002 [00:00<00:00, 22154.56 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'targets', 'language', 'language_code', 'annotation_type', 'user_id'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['inputs', 'targets', 'language', 'language_code', 'annotation_type', 'user_id'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_text = aya_nepali.filter(lambda example: example['inputs'] == \"\" or example['targets'] == \"\")\n",
    "empty_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14.914042978510745, 40.942278860569715)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_inputs_len = sum(len(text.split()) for text in aya_nepali['train']['inputs']) / len(aya_nepali['train']['inputs'])\n",
    "avg_outputs_len = sum(len(text.split()) for text in aya_nepali['train']['targets']) / len(aya_nepali['train']['targets'])\n",
    "(avg_inputs_len, avg_outputs_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'targets', 'language', 'language_code', 'annotation_type', 'user_id'],\n",
       "        num_rows: 1153\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['inputs', 'targets', 'language', 'language_code', 'annotation_type', 'user_id'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aya_hindi = aya_dataset.filter(lambda example: example['language'] == \"Hindi\")\n",
    "aya_hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "दाल मखनी कैसे बनाते हैं?\n"
     ]
    }
   ],
   "source": [
    "print(aya_hindi['train']['inputs'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "दाल मखनी बनाने के लिए सबसे पहले काली उरद दाल और राजमा को रात भर भिगो दें। फिर इसे प्रेशर कुकर में पकाएं। एक पैन में मक्खन गरम करें, जीरा, अदरक-लहसुन का पेस्ट, प्याज, टमाटर, मिर्च पाउडर, और गरम मसाला डालकर भुनें। पकी हुई दाल और राजमा को इसमें मिलाएं और धीमी आंच पर पकने दें। क्रीम मिलाकर गाढ़ा होने तक पकाएं।\n"
     ]
    }
   ],
   "source": [
    "print(aya_hindi['train']['targets'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter:   0%|          | 0/1153 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 1153/1153 [00:00<00:00, 18545.56 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'targets', 'language', 'language_code', 'annotation_type', 'user_id'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['inputs', 'targets', 'language', 'language_code', 'annotation_type', 'user_id'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_text = aya_hindi.filter(lambda example: example['inputs'] == \"\" or example['targets'] == \"\")\n",
    "empty_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23.617519514310494, 80.50910667823071)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_inputs_len = sum(len(text.split()) for text in aya_hindi['train']['inputs']) / len(aya_hindi['train']['inputs'])\n",
    "avg_outputs_len = sum(len(text.split()) for text in aya_hindi['train']['targets']) / len(aya_hindi['train']['targets'])\n",
    "(avg_inputs_len, avg_outputs_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 4002 and 1153 samples for Nepali and Hindi, respectively. We can do few-shot experiments with those. We will have to see if it is enough for LoRA. Else, we will use Alpaca datasets translated into Nepali and Hindi.\n",
    "\n",
    "### Alpaca Dataset\n",
    "\n",
    "Alpaca Dataset is 52K instruction-following data by Stanford. We will be using the translated version of [Alpaca Clean](https://huggingface.co/datasets/yahma/alpaca-cleaned): \n",
    "- [Nepali](https://huggingface.co/datasets/Saugatkafley/alpaca-nepali-sft)\n",
    "- [Hindi](https://huggingface.co/datasets/iamshnoo/alpaca-cleaned-hindi) - Translated from yahma/alpaca-cleaned using NLLB-1.3B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 384/384 [00:00<00:00, 1.82MB/s]\n",
      "Downloading data: 100%|██████████| 18.9M/18.9M [00:02<00:00, 8.31MB/s]\n",
      "Generating train split: 100%|██████████| 52005/52005 [00:00<00:00, 327891.81 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'id'],\n",
       "        num_rows: 52005\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_nepali = load_dataset(\"Saugatkafley/alpaca-nepali-sft\")\n",
    "alpaca_nepali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "तीन प्राथमिक रंगहरू के हुन्?\n"
     ]
    }
   ],
   "source": [
    "print(alpaca_nepali['train']['instruction'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(alpaca_nepali['train']['input'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "तीन प्राथमिक रंगहरू रातो, नीलो र पहेंलो हुन्।\n"
     ]
    }
   ],
   "source": [
    "print(alpaca_nepali['train']['output'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter:   0%|          | 0/52005 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 52005/52005 [00:00<00:00, 79630.14 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'id'],\n",
       "        num_rows: 31337\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_text = alpaca_nepali.filter(lambda example: example['input'] == \"\" or example['output'] == \"\" or example['instruction'] == \"\")\n",
    "empty_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 52005/52005 [00:00<00:00, 91364.42 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'id'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_text = alpaca_nepali.filter(lambda example: example['instruction'] is None)\n",
    "empty_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter:   0%|          | 0/52005 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 52005/52005 [00:00<00:00, 76304.33 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'id'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_text = alpaca_nepali.filter(lambda example: example['instruction'] == \"\" or example['instruction'] is None)\n",
    "empty_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'id'],\n",
       "        num_rows: 42\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_text = alpaca_nepali.filter(lambda example: example['output'] == \"\" or example['output'] is None)\n",
    "empty_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter:   0%|          | 0/51963 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 51963/51963 [00:01<00:00, 40199.22 examples/s]\n",
      "Filter: 100%|██████████| 51963/51963 [00:01<00:00, 40964.07 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'id'],\n",
       "        num_rows: 51959\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_nepali_filtered = alpaca_nepali.filter(lambda example: example['output'] is not None and example['output'] != \"\")\n",
    "alpaca_nepali_filtered = alpaca_nepali_filtered.filter(lambda example: example['instruction'] != \"\" and example['instruction'] is not None)\n",
    "alpaca_nepali_filtered = alpaca_nepali_filtered.filter(lambda example: example['input'] is not None)\n",
    "alpaca_nepali_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.68538655478358, 3.4686579803306454, 38.96822494659251)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_inst_len = sum(len(text.split()) for text in alpaca_nepali_filtered['train']['instruction']) / len(alpaca_nepali_filtered['train']['instruction'])\n",
    "avg_inputs_len = sum(len(text.split()) for text in alpaca_nepali_filtered['train']['input']) / len(alpaca_nepali_filtered['train']['input'])\n",
    "avg_outputs_len = sum(len(text.split()) for text in alpaca_nepali_filtered['train']['output']) / len(alpaca_nepali_filtered['train']['output'])\n",
    "(avg_inst_len, avg_inputs_len, avg_outputs_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 497/497 [00:00<00:00, 1.36MB/s]\n",
      "Downloading data: 100%|██████████| 31.3M/31.3M [00:05<00:00, 5.60MB/s]\n",
      "Generating train split: 100%|██████████| 51760/51760 [00:00<00:00, 290897.61 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'instruction', 'output'],\n",
       "        num_rows: 51760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_hindi = load_dataset(\"iamshnoo/alpaca-cleaned-hindi\")\n",
    "alpaca_hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "तीन प्राथमिक रंग क्या हैं?\n"
     ]
    }
   ],
   "source": [
    "print(alpaca_hindi['train']['instruction'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(alpaca_hindi['train']['input'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "प्रकाश के लिए उपयोग किए जाने वाले योजक रंग प्रणाली में, प्राथमिक रंग लाल, हरे और नीले (आरजीबी) हैं। प्रकाश के लिए उपयोग किए जाने वाले योजक रंग प्रणाली में, प्राथमिक रंग लाल, हरे और नीले (आरजीबी) हैं।\n"
     ]
    }
   ],
   "source": [
    "print(alpaca_hindi['train']['output'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter:   0%|          | 0/51760 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 51760/51760 [00:01<00:00, 49712.86 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'instruction', 'output'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_text = alpaca_hindi.filter(lambda example: example['input'] is None)\n",
    "empty_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 51760/51760 [00:00<00:00, 64501.34 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'instruction', 'output'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_text = alpaca_hindi.filter(lambda example: example['instruction'] == \"\" or example['instruction'] is None)\n",
    "empty_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 51760/51760 [00:00<00:00, 84864.02 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'instruction', 'output'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_text = alpaca_hindi.filter(lambda example: example['output'] == \"\" or example['output'] is None)\n",
    "empty_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.920595054095827, 4.6264296754250385, 112.07911514683153)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_inst_len = sum(len(text.split()) for text in alpaca_hindi['train']['instruction']) / len(alpaca_hindi['train']['instruction'])\n",
    "avg_inputs_len = sum(len(text.split()) for text in alpaca_hindi['train']['input']) / len(alpaca_hindi['train']['input'])\n",
    "avg_outputs_len = sum(len(text.split()) for text in alpaca_hindi['train']['output']) / len(alpaca_hindi['train']['output'])\n",
    "(avg_inst_len, avg_inputs_len, avg_outputs_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
